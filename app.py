print("üí° Fichier app.py bien modifi√© !")
# ----------------------------------------
# üåü Assistant IA Empathique - app.py
# ----------------------------------------

# Imports essentiels pour ton app
import streamlit as st
from transformers import pipeline, AutoTokenizer as FalconTokenizer, AutoModelForCausalLM as FalconModel
import torch  # üî• Garde torch uniquement car il est utilis√© pour Falcon



from transformers import pipeline

# On charge un mod√®le public d√©j√† entra√Æn√© sur les √©motions
emotion_classifier = pipeline("text-classification", model="bhadresh-savani/distilbert-base-uncased-emotion")

# ‚úî Nouvelle fonction de d√©tection :
def detect_emotion(text):
    result = emotion_classifier(text)[0]
    return result["label"].lower()

# üìå Liste des √©tiquettes d‚Äô√©motions
emotion_labels = ["sadness", "joy", "love", "anger", "fear", "surprise"]

# ----------------------------------------
# ü§ñ Moteur de r√©ponse empathique - Falcon 7B
# ----------------------------------------

# from transformers import AutoTokenizer as FalconTokenizer
# from transformers import AutoModelForCausalLM as FalconModel

# # Charger Falcon 7B Instruct
# falcon_model_name = "tiiuae/falcon-7b-instruct"
# falcon_tokenizer = FalconTokenizer.from_pretrained(falcon_model_name)



# import os
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"  # ‚úÖ Prevent CUDA OOM

# from transformers import AutoTokenizer as FalconTokenizer
# from transformers import AutoModelForCausalLM as FalconModel

# falcon_model_name = "tiiuae/falcon-7b-instruct"

# falcon_tokenizer = FalconTokenizer.from_pretrained(falcon_model_name)
# falcon_model = FalconModel.from_pretrained(
#     falcon_model_name,
#     device_map="auto",  # ‚úÖ Spread layers across GPU and CPU if needed
#     trust_remote_code=True,  # ‚úÖ Required for Falcon
#     torch_dtype=torch.float16  # ‚úÖ Half precision to save GPU memory
# )

from transformers import AutoTokenizer as FalconTokenizer
from transformers import AutoModelForCausalLM as FalconModel
import torch

falcon_model_name = "tiiuae/falcon-7b-instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"

falcon_tokenizer = FalconTokenizer.from_pretrained(falcon_model_name)
falcon_model = FalconModel.from_pretrained(
    falcon_model_name,
    device_map="auto",            # Spread across GPU/CPU
    torch_dtype=torch.float16,    # Use FP16 to save VRAM
    trust_remote_code=True,
    low_cpu_mem_usage=True        # Optimize RAM usage
)

falcon_model.eval()

# !pip install accelerate

# Fonction pour g√©n√©rer une r√©ponse empathique
def generate_empathic_response_llm(emotion_label):
    prompt = (
        f"You are a kind, caring and empathetic assistant. "
        f"A user is feeling {emotion_label}. "
        f"Write a short, warm and emotionally supportive sentence."
    )

    inputs = falcon_tokenizer(prompt, return_tensors="pt").to(device)
    outputs = falcon_model.generate(
        **inputs,
        max_length=60,
        do_sample=True,
        top_k=50,
        top_p=0.9,
        temperature=0.7,
        pad_token_id=falcon_tokenizer.eos_token_id
    )

    response = falcon_tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Nettoyer la sortie
    if prompt in response:
        response = response.replace(prompt, "").strip()
    return response.strip()

# ----------------------------------------
# üéØ Fonction de d√©tection d‚Äô√©motion
# ----------------------------------------

# def detect_emotion(text):
#     inputs = emotion_tokenizer(text, return_tensors="pt", truncation=True, padding=True)
#     inputs = {k: v.to(emotion_model.device) for k, v in inputs.items()}
#     outputs = emotion_model(**inputs)
#     predicted_label = torch.argmax(outputs.logits, dim=1).item()
#     return emotion_labels[predicted_label]

# ----------------------------------------
# üíª Interface utilisateur Streamlit
# ----------------------------------------

st.set_page_config(page_title="IA Empathique", page_icon="ü§ñ")
st.title("üí¨ Assistant IA Empathique")
st.markdown("Entrez un message ci-dessous. L'IA d√©tectera votre √©motion et vous r√©pondra avec bienveillance.")

# Zone de texte utilisateur
user_input = st.text_area("‚úçÔ∏è Votre message :", height=150)

# Bouton de validation
if st.button("Analyser & R√©pondre"):
    if not user_input.strip():
        st.warning("‚õî Veuillez √©crire un message.")
    else:
        with st.spinner("üß† Analyse de l'√©motion..."):
            emotion = detect_emotion(user_input)
            response = generate_empathic_response_llm(emotion)

        # Affichage des r√©sultats
        st.success(f"üß† √âmotion d√©tect√©e : **{emotion.upper()}**")
        st.info(f"ü§ñ R√©ponse de l'IA : *{response}*")
